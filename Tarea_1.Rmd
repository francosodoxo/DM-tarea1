---
title: "Tarea 1: Exploración y Visualización de Datos"
author: "Felipe Bravo, Hernán Sarmiento, Aymé Arango, Alison Fernandez, Cinthia Mabel Sanchez, Juan Pablo Silva"
date: "Septiembre 2020"
output:
  html_document:
    number_sections: yes
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---

# Declaración de compromiso ético
Nosotros **Franco Seguel y Álvaro Becerra**, declaramos que realizamos de manera grupal los pasos de la presente actividad. También declaramos no incurrir en copia, ni compartir nuestras respuestas con otras personas ni con otros grupos. Por lo que, ratificamos que las respuestas son de nuestra propia confección y reflejan nuestro propio conocimiento.

# Instrucciones

1. Trabajen en equipos de dos o tres personas. Salvo excepciones, no se corregirá entregas con menos de dos integrantes.

2. Modifique este archivo `.Rmd` agregando sus respuestas donde corresponda.

3. Para cada pregunta, cuando corresponda, **incluya el código fuente que utilizó para llegar a su respuesta**.

4. El formato de entrega para esta actividad es un archivo html. **Genere un archivo HTML usando RStudio** y súbalo a U-Cursos.
   Basta con que uno de los integrantes haga la entrega. Si ambos hacen una entrega en U-Cursos, se revisará cualquiera de éstas.

# Tarea 
La primera parte de esta actividad son preguntas teóricas que avanzaron en las clases del curso de Minería de datos.

## Teoría

*1. ¿Cuál es el objetivo de Minería de datos y qué la diferencia de Machine Learning? De un ejemplo.*
 
**Respuesta:**
El objetivo de la minería de datos es generar conocimiento útil a partir de grandes sets de datos que no necesariamente fueron estructurados para los fines de estudio. Se diferencia de Maching Learning en que esta última disciplina se enfoca en predecir a través de algoritmos automodificables. Un ejemplo en el contexto de los videosjuegos es el uso de machine learning para contrincantes artificiales (los bots) que deben ser capaces de predecir los movimientos del jugador para no ser un blanco fácil, a la vez que permiten ganar al jugador para motivarlo a seguir. Data Mining se puede utilizar recopilando toda la información de los chats para lograr concluir que los jugadores desean un mouse con determinada cantidad de botones.  

*2. Describa y compare los siguientes métodos usados en Minería de datos: clasificación vs. clustering.*

**Respuesta:**
Clasificación busca predecir la categoría de un nuevo dato utilizando algoritmos supervisados, en donde se utiliza un conjunto Ground Truth para entrenar y un un Conjunto de verificación para evaluar el desempeño del clasificador. En cambio, Clustering tiene la finalidad de encontrar grupos con características comunes (como por ejemplo distancia euclideana) dentro de un dataset, de esta forma se puede segmentar un conjunto de datos.  

*3. ¿Qué desafios existen en Minería de datos?*

**Respuesta:**
El primer deafío consiste en la escalabilidad, consistente en desarrollar modelos capaces de funcionar con datasets de tamaños pequeños y grandes. El segundo sefío corresponden a la dimensionalidad, ya que se debe buscar una dimensión óptima que permita crear un modelo fuerte a sin llegar a ser demasiado complejo de computar. El tercer desafío es la obtención de datos, ya que estos suelen ser complejos y heterogéneos, ya que por lo general son recolectados con finalidades diferentes a las del modelo, debiendo utilizar de preprocesamiento. El cuarto desafío tiene que ver con la poca calidad de los datasets los cuales deben ser procesados para eliminar ruido, outliers y datos incompletos. El quinta desafío corresponde a la distribución de datos y propiedad, un caso contigente es la distribución de datos de la evolución de la pandemia en Chile donde el Minsal sólo ha circulado informes en formato PDF obstaculizando el proceso de obtención de datos para su posterior análisis por parte de la comunidad civil. En sexto lugar se tiene como desafío proteger la privacidad de los datos, ya que estos pueden contener información sensible (informes médicos, antecedentes penales, etc.), en consecuencia, se pueden anonimizar los datos sensibles para su análisis público. El último, corresponde al streaming, donde se debe abordar un flujo de información de gran dimensionalidad con una nula estructura, como la emisión de videos.   

*4. Respecto a los tipos de atributo, ¿cuál es la diferencia entre razón e intervalo?*

**Respuesta:**
La diferencia entre razón e intervalo consiste en la existencia de una referencia fija que poseen los intervalos, como por ejemplo las dimensiones espaciales como largo, ancho, etc. Un intervalo carece de esta referencia absoluta, un ejemplo de interválo es la temperatura en Celsius, mientra que la temperatura en Kelvin corresponde a una razón por tener la referencia del 0 absoluto.


*5. ¿Qué factores que ocasionan errores en el análisis de datos deben ser considerados para la limpieza de un set de datos?*

**Respuesta:**
Un factor a considerar es el sesgo del dataset, por ejemplo si se concluye una preferencia de mercado a través de un dataset de twitter, esta conclusión no necesariamente aplicará a grupos que no utilizan redes sociales como los adultos mayores. Otro factor a considerar es la calibración de los intrumentos de obtención de datos, en donde las conclusiones pueden ser erróneas debido a la fuente. Otro factor a considerar es la distribución de los datos de entrada, ya que se deben distribuir de acuerdo a los propositos de los diferentes trabajos evitando categorías sobrerrepresentadas o subrepresentadas. Otro factor es la dimesnión del dataset que debe ser lo suficientemente grande para generar el modelo y lo suficientemente pequeño para ser abordable desde un computador. 

*6. ¿Qué es el análisis exploratorio de datos o EDA?*

**Respuesta:**
Corresponde al análisis estadístico básico que se le puede hacer a un data set con la finalidad de reconocer anomalías que motiven estudios, incluyendo obtención de medianas, medias, percentiles, máximos, mínimos y correlaciones. También permite conocer la calidad del dataset motivando acciones de preprocesamiento. 

*7. Describa las medidas de tendencia central: media y mediana. Exponga la diferencia entre ambas.*

**Respuesta:**
La media corresponde al promedio de los datos numéricos mientras que la mediana es el dato que divide un conjunto de datos en dos percentiles (percentil 50). La media de un conjunto es altamente sensible a datos extremos o fuera de rango, mientras que la mediana de un conjunto es más robusta frente a outliers. Cuando los datos se distribuyen de manera uniforme la media y la mediana se encuentran muy próximas, mientras que para otros conjuntos de datos la mediana puede diferir agudamente de la media. La relación entre media y mediana entrega información sobre los outliers del conjunto de datos. 

*8. ¿Qué es una matriz de correlación y para qué sirve?*

**Respuesta:**
La matriz de correción es la matriz contruida con los coeficientes de correlación que tienen dos variables (columna y fila de la matriz), la diagonal de esta matriz corresponde a una diagonal de unos. Sirve para analizar las dependencias que existen entre las dos variables columna y fila, si la correlación es cercana a 1 existe una razón de proporcionalidad directa, si es nula, no existe una relación numérica entre las variables, si la correlación es cercana a -1, entonces las variables son inversamente proporcionales. El detectar este tipo de relaciones puede generar motivación en el estudio de un dataset teniendo siempre en cuenta que correlación no implica causalidad, es decir, que no porque los datos se ajusten a un modelo, entonces están estrechamente relacionados.

*9. Explique cómo se construye un Boxplot y exponga cómo se identifican los valores atípicos u outliers en este tipo de gráfico.*

**Respuesta:**
El boxplot se construye con los tres percentiles más importantes de un dataset, en primer lugar el percentil 50 marca el centro de la caja, luego los percentiles 25 y 75 se ubican en el limite inferior y superior de la caja, luego los brazos corresponden a una al percentil superior más una proporcion de la razón inter cuartil, mientras que el límite del brazo inferior corresponde al primer cuartil menos la misma proprcion de la razón intercuartil. Esta forma de ilustrar los datos permite obtener un resumen de cómo se encuentran distribuidos, teniendo en mente que una distribución normal la mediana se encuentra en el centro del boxplot así como la caja, siendo un dibujo simétrico con respecto a la mediana.

## Práctica 

En esta parte de la actividad se trabajará con los datos del Proceso Constituyente 2016-2017 publicados en el Portal de Datos Abiertos del Gobierno de Chile, para mayor información pueden ingresar al siguiente link: https://datos.gob.cl/dataset/proceso-constituyente-abierto-a-la-ciudadania. Los datos corresponden a las actas de los Encuentros Locales Autoconvocados (ELAs), en cada cual, un grupo de personas se reune a discutir distintos conceptos como por ejemplo: salud, educación, seguridad, etc.

Los datos con los que trabajarán consisten en la cantidad de veces que cada concepto constitucional fue mencionado por cada localidad de Chile. 

Para cargar los datos, use:

```{r}
data_tf <- read.csv("http://dcc.uchile.cl/~hsarmien/mineria/datasets/actas.txt", header = T, fileEncoding = "UTF-8")
```

**Por cada pregunta adjunte el código R que utilizó para llegar a la respuesta. Respuestas sin código no recibirán puntaje**

### Exploración básica

1. ¿Cuáles son las dimensiones del dataset (filas, columnas)? Adjunte código o indique cómo determinó la cantidad de datos total. 

```{r}
# RESPUESTAselec
dim(data_tf)
```
R: Las dimensiones del dataset son 328 filas y 114 columnas.
2. ¿Qué describe cada línea del dataset? (ejemplifique tomando la fila 85)

```{r}
# RESPUESTA
# No se incluyen todas las columnas para hacer el html legible
str(data_tf[85,1:10])
```
R: Cada línea del dataset está compuesto por 113 columnas donde la primera corresponde a la localidad (tipo factor) y las siguientes 112 corresponden al número de veces que se mencionó el concepto de la etiqueta de la columna respectiva. Tomando como ejemplo la fila 85, se nos muestra aque en la localidad de Aysen la "igualdad ante la ley" se mencionó 3 veces.

3. ¿Existen localidades repetidas en el dataset? Adjunte el código o indique cómo llegó a esa conclusión. 

```{r}
# RESPUESTA
length(unique(data_tf$localidad))
```
La función unique entrega 328 valores diferentes que coinciden con la cantidad de filas del dataset concluyendo que no existen datos duplicados. Sin enbargo la función unique opera bajo el supuesto de que los datos se encuentran bien ingresados, errores en tildes o codificación que pudiesen significar duplicación no están siendo analizados.

4. Liste los nombres de las columnas del dataset `data_tf`. Adjunte código en R y *recuerde* usar `head` si el resultado es muy largo.

```{r}
# RESPUESTA
# Las primeras 20 columnas 
names(data_tf)[1:20]
```


### Análisis

1. Liste todas las localidades donde *no* se discu]1:15tió el concepto `a_la_salud`. 

```{r}
# RESPUESTA
head(data_tf[data_tf$a_la_salud == 0,1])

```

2. Liste las 10 localidades que más mencionaron el concepto `justicia`. 

```{r}
# RESPUESTA
data_tf[order(data_tf$justicia, decreasing=TRUE),1][1:10]
```


3. Liste los 10 conceptos menos mencionados a lo largo de todo el proceso.


```{r}
# RESPUESTA
colSums(data_tf[,2:113])[order(colSums(data_tf[,-1]), decreasing = FALSE)][1:10]

```


4. Liste las 10 localidades que más participaron en el proceso. Describa cómo definió su medida de participación.


```{r}
# Se suman todas las filas y las 10 localidades que tienen mayor valor se muestran.
newData <- apply(data_tf[,3:113],1,sum)
dataf <- data.frame(localidad = data_tf$localidad, total = newData)
dataf[order(dataf$total, decreasing = TRUE),1][1:10]
```

5. Ejecute el siguiente código que permitirá agregar una nueva columna a nuestro dataframe que solo tendrá el nombre de la región.

```{r, message = F, warning=F}
library(dplyr)
regiones <- strsplit(as.character(data_tf[,1]), '/')
data_tf$region <- sapply(regiones, "[[", 1)
data_tf <- data_tf %>% dplyr::select(localidad, region, everything())
```

Luego, genere un gráfico de barras (ggplot) que muestre los 10 conceptos más mencionados en cada una de las regiones mencionadas (adjunte gráficos y código):

- `Atacama`
- `Coquimbo`
- `Metropolitana de Santiago`


Cabe resaltar, que se esperan tres gráficos de barras para las tres diferentes regiones:

```{r}
# 10 conceptos más mencionados en Atacama

pplot <- function(region){
  library(ggplot2)
  cols <- colnames(data_tf)[3:113]
  numeros <- apply(data_tf[data_tf$region==region,3:113],2,sum)
  
  newDf <- data.frame(conceptos = cols, frec = numeros)
  newDf<- newDf[order(newDf$frec, decreasing = TRUE),][1:10,]
  ggplot(newDf)+
    geom_bar(aes(x = conceptos, y= frec),stat="identity")+
    ggtitle(paste("Conceptos más mencionados en",region,sep=' '))+
    coord_flip()+
    xlab('Conceptos')+
    ylab('Frecuencia')
}

pplot('Atacama')

#dataa <- data_tf[data_tf$region == 'Atacama'][order(data$)]
#ggplot2::ggplot()
```

```{r}
# 10 conceptos más mencionados en Coquimbo
pplot('Coquimbo')
```

```{r}
# 10 conceptos más mencionados en Metropolitana de Santiago
pplot('Metropolitana de Santiago')
```

6. De la pregunta anterior, ¿considera que es razonable usar el conteo de frecuencias para determinar las regiones que tuvieron mayor participación en el proceso? ¿Por qué? Sugiera y solamente comente una forma distinta de hacerlo.

```{r}
# RESPUESTA
print("No es una buena sugerencia, porque no se sabe qué tan profundas fueron las discusiones que se dieron en cada localidad, entonces una medida a considerar sería haber agregado el tiempo que se demoraron en discutir cada punto y medir en cuál localidad se dieron discusiones más largas.")
```

## Ejercicios

### Accidentes de tránsito

Para esta sección utilizaremos un dataset real de número de accidentes de tránsito por localidad, el cual puede ser encontrado en el siguiente link: http://datos.gob.cl/dataset/9348. Para cargar el dataset ejecute el siguiente código:

```{r}
tipos <- read.table("https://users.dcc.uchile.cl/~hsarmien/mineria/datasets/accidentes_2010_2011.txt", fileEncoding="UTF-8")
head(tipos)
```

Explore el set de datos para responder las siguientes preguntas:

1. Filtre los datos para incluir sólo los accidentes ocurridos el año 2011 a nivel regional. Genere un boxplot donde se indique la cantidad de accidentes categorizado por tipo de accidente.

Este tipo de gráfico nos ayudará a entender como se distribuye los datos por cada tipo de accidentes. Es decir, podremos apreciar que tan dispersos o similares son los datos en todo el dataset. También, puede ser útil para observar valores atípicos u outliers en los datos.

```{r message=FALSE, warning=FALSE}
# RESPUESTA 
library(tidyverse)
# OJO CON CANTIDAD DE IMPORTACIONES DE TIDYVERSE
ac_2011<-tipos %>%
  filter(Anio==2011 & Muestra=="Regional")%>%
  group_by(TipoAccidente) #%>%
  #summarise(Total = sum(Cantidad))
plot(ac_2011$TipoAccidente,ac_2011$Cantidad, xlab = "Tipo de Accidente", ylab="Total de accidentes", main="Cantidad de accidentes por tipo para el año 2011")
plot(ac_2011$TipoAccidente,ac_2011$Cantidad, xlab = "Tipo de Accidente", ylab="Total de accidentes", main="Cantidad de accidentes por tipo para el año 2011",ylim
     =c(0,4000))
```

2. ¿Qué aprecia con el gráfico anterior? ¿Qué otra forma de explorar los datos podría agregar? ¿Qué información adicional aporta? Adjunte una breve explicación.

En primer lugar se aprecian outliers, por esta razón se modifican los límites del eje vertical para lograr una mejor comprensión de los boxplots. En el segundo gráfico se observa que los datos de accidentes a nivel regional agrupados por tipo no satisfacen una distribución normal, los brazos superior e inferior no logran ser simétricos para ningún tipo de accidente ni la mediana se encuentra en el centro de los boxplots. Lo observado se debe a que se están graficando los datos absolutos a nivel regional sin considerar la población por región, en consecuencia, regiones como la Metropolitana concentran la mayor cantidad de accidentes y puede aparecer como outliers. Una forma de corregir este problema es dividir la cantidad de accidentes por el total regional.

```{r}
new_ac_2011<-merge(ac_2011%>%group_by(Descripcion)%>%summarise(total_reg = sum(Cantidad)),ac_2011, by="Descripcion")
cant_per_total<-c(new_ac_2011$Cantidad/new_ac_2011$total_reg)
new_ac_2011<-cbind(new_ac_2011,cant_per_total)
new_acc_2011_g<-new_ac_2011 %>%
  group_by(TipoAccidente)
plot(new_acc_2011_g$TipoAccidente,new_acc_2011_g$cant_per_total, xlab = "Tipo de Accidente", ylab="Total de accidentes", main="Accidentes por tipo  normalizados por total regional (2011)")

```


**Respuesta:**
Como se logra ver en el gráfico, la normalización por total de accidentes a nivel regional logra reducir los outliers, aún así, los datos se distribuyen de manera poco simétrica al no coincidir la mediana en el centro de los boxplots. 

### Diamantes

Considere el set de datos diamonds del paquete ggplot2 de R, que contiene los precios en dolares, junto con otros atributos importantes: quilates, corte, color y claridad. También hay medidas físicas como ser: x (largo), y (ancho), z (profundidad), depth (porcentaje total de profundidad) y table (ancho desde el tope del diamante al punto relativo más ancho del diamante):

```{r}
data("diamonds")
# la siguiente linea genera un error en el knit
# diamonds[1:10,]
```

Realice una exploración por el set de datos para responder las siguientes preguntas:

1. Teniendo en cuenta las medidas físicas, ¿considera que existen valores inexistentes o inconsistentes? Describa como manejaría esta situación. Adjunte el código necesario.

```{r}
# RESPUESTA
# Para ver datos inexistentes, se usa lo siguente
sapply(diamonds, function(x) sum(is.na(x)))
 ```

```{r}
#para graficar se usa el código a continuación
datad <- data.frame(cut = diamonds$cut, depth = diamonds$depth)
ggplot(datad,aes(x= cut, y=depth))+geom_boxplot()
```
No hay valores inexistentes como se muestra en la figura anterior, pero hay varios outliers, que se calculan a continuación, obteniendo la cantidad de datos que están fuera del intervalo [1er cuartil, 3er cuartil] de los datos.

```{r}
outliers <- function(x,inf,sup){
  addition <- 0
  for (var in x){
    if (var < inf || var > sup){
      addition <- addition + 1
    }
  }
  return(addition)
}
```
Se calculan el porcentaje de outliers que tiene una característica de la siguiente forma: -->
```{r}
summary(diamonds[diamonds$cut=='Fair',])
fair_frame <- diamonds[diamonds$cut == "Fair",]
print("El porcentaje de outliers de la característica depth del grupo 'Fair' es:")
print((outliers(fair_frame$depth,64.4,65.9)/length(fair_frame$depth))*100)
```
2. Considerando la relación entre dos atributos, ¿qué atributos están más correlacionadas con el precio (price) y qué significa esto? ¿cuál es la correlación más alta para table? Adjunte el código necesario para la respuesta.

```{r}
# RESPUESTA
print("depth vs price")
cor(fair_frame$depth, fair_frame$price)

print("table vs price")
cor(fair_frame$table, fair_frame$price)

print("x vs price")
cor(fair_frame$x, fair_frame$price)

print("y vs price")
cor(fair_frame$y, fair_frame$price)

print("z vs price")
cor(fair_frame$z, fair_frame$price)
```
El tamaño del diamante (x,y,z) está fuertemente relacionado con el precio de este.

La correlación más alta para table está dada por:

```{r}
data_good <- diamonds[diamonds$cut == 'Good',]
data_very_good <- diamonds[diamonds$cut == 'Very Good',]
data_premium <- diamonds[diamonds$cut == 'Premium', ]
data_ideal<- diamonds[diamonds$cut=='Ideal',]
summary(data_good$table)
c1 <- cor(fair_frame$table, fair_frame$price)
c2 <- cor(data_good$table, data_good$price)
c3 <- cor(data_very_good$table, data_very_good$price)
c4 <- cor(data_premium$table, data_premium$price)
c5 <- cor(data_ideal$table, data_ideal$price)
datos <- c(c1,c2,c3,c4,c5)
print(abs(datos))
```

El cut good está más correlacionado con el precio en la variable table.
 
3. Proponga otra forma para explorar los datos. ¿Qué información adicional aporta? Adjunte una breve explicación.
 
**Respuesta:**

Una forma puede ser cómo se relacionan las variables del tamaño del diamante con su depth y table, graficando y ver alguna relación visual si es que existe. Otra forma es el diagrama de rostros que permite graficar datos multidimensionales en una sola imagen.